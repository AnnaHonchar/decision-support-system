from flask import Blueprint, jsonify
import pandas as pd

api = Blueprint("api", __name__)

@api.route("/ping", methods=["GET"])
def ping():
    return jsonify({"message": "pong!"})

from flask import request, jsonify
from werkzeug.security import generate_password_hash, check_password_hash
from app.models import User
from app.db import db
from app.models import Recommendation

@api.route("/register", methods=["POST"])
def register():
    data = request.json
    username = data.get("username")
    email = data.get("email")
    password = data.get("password")

    if not username or not email or not password:
        return jsonify({"error": "–í—Å—ñ –ø–æ–ª—è –æ–±–æ–≤‚Äô—è–∑–∫–æ–≤—ñ"}), 400

    if User.query.filter((User.username == username) | (User.email == email)).first():
        return jsonify({"error": "–ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –∑ —Ç–∞–∫–∏–º —ñ–º‚Äô—è–º –∞–±–æ email –≤–∂–µ —ñ—Å–Ω—É—î"}), 409

    hashed_password = generate_password_hash(password)
    user = User(username=username, email=email, password=hashed_password)
    db.session.add(user)
    db.session.commit()

    return jsonify({"message": "–ö–æ—Ä–∏—Å—Ç—É–≤–∞—á —É—Å–ø—ñ—à–Ω–æ –∑–∞—Ä–µ—î—Å—Ç—Ä–æ–≤–∞–Ω–∏–π"}), 201

@api.route("/login", methods=["POST"])
def login():
    data = request.json
    email = data.get("email")
    password = data.get("password")

    if not email or not password:
        return jsonify({"error": "–ó–∞–ø–æ–≤–Ω—ñ—Ç—å —É—Å—ñ –ø–æ–ª—è"}), 400

    user = User.query.filter_by(email=email).first()

    if not user or not check_password_hash(user.password, password):
        return jsonify({"error": "–ù–µ–≤—ñ—Ä–Ω–∏–π email –∞–±–æ –ø–∞—Ä–æ–ª—å"}), 401

    return jsonify({
        "message": "–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è —É—Å–ø—ñ—à–Ω–∞",
        "username": user.username,
        "user_id": user.id
    }), 200

import os
from datetime import datetime
from flask import current_app
from werkzeug.utils import secure_filename
from app.models import Dataset

@api.route("/upload", methods=["POST"])
def upload_file():
    if 'file' not in request.files:
        return jsonify({"error": "–§–∞–π–ª –Ω–µ –Ω–∞–¥—ñ—Å–ª–∞–Ω–æ"}), 400

    file = request.files['file']
    user_id = request.form.get('user_id')

    if not user_id:
        return jsonify({"error": "–ù–µ –≤–∫–∞–∑–∞–Ω–æ ID –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞"}), 400

    if file.filename == '':
        return jsonify({"error": "–§–∞–π–ª –Ω–µ –≤–∏–±—Ä–∞–Ω–æ"}), 400

    filename = secure_filename(file.filename)
    upload_path = os.path.join(current_app.config['UPLOAD_FOLDER'], filename)
    file.save(upload_path)

    dataset = Dataset(
        user_id=user_id,
        filename=filename,
        upload_date=datetime.utcnow()
    )
    db.session.add(dataset)
    db.session.commit()

    return jsonify({"message": "–§–∞–π–ª –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ", "dataset_id": dataset.id}), 200

from app.ml import train_and_predict
from app.models import Dataset, Prediction, ClassificationResult
import os
from flask import current_app
from datetime import datetime

@api.route("/analyze", methods=["POST"])
def analyze():
    data = request.json
    dataset_id = data.get("dataset_id")

    if not dataset_id:
        return jsonify({"error": "–ù–µ –≤–∫–∞–∑–∞–Ω–æ dataset_id"}), 400

    dataset = Dataset.query.get(dataset_id)

    if not dataset:
        return jsonify({"error": "–ù–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"}), 404

    filepath = os.path.join(current_app.config["UPLOAD_FOLDER"], dataset.filename)

    if not os.path.exists(filepath):
        return jsonify({"error": "–§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"}), 404

    try:
        result = train_and_predict(filepath)

        rmse = result["rmse"]
        r2 = result["r2_score"]
        mae = result["mae"]

        # –§–æ—Ä–º—É–≤–∞–Ω–Ω—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ RMSE
        if rmse <= 10:
            recommendation_text = "–ü—Ä–æ–≥–Ω–æ–∑ –¥–æ—Å–∏—Ç—å —Ç–æ—á–Ω–∏–π. –ú–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –¥–ª—è –ø—Ä–∏–π–Ω—è—Ç—Ç—è —Ä—ñ—à–µ–Ω—å."
        elif rmse <= 25:
            recommendation_text = "–ü—Ä–æ–≥–Ω–æ–∑ –ø–æ–º—ñ—Ä–Ω–æ—ó —Ç–æ—á–Ω–æ—Å—Ç—ñ. –†–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–∏—Ö."
        else:
            recommendation_text = "–ù–∏–∑—å–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—É. –°–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∫—Ä–∞—â–∏—Ç–∏ —è–∫—ñ—Å—Ç—å –∞–±–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö."

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –≤ predictions
        prediction = Prediction(
            dataset_id=dataset_id,
            created_at=datetime.utcnow(),
            result_summary=f"RMSE: {rmse:.2f}, R¬≤: {r2:.2f}, MAE: {mae:.2f}"
        )
        db.session.add(prediction)
        db.session.commit()

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
        from app.models import Recommendation
        rec = Recommendation(
            prediction_id=prediction.id,
            text=recommendation_text,
            created_at=datetime.utcnow()
        )
        db.session.add(rec)
        db.session.commit()

        return jsonify({
            "message": "–ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ",
            "rmse": rmse,
            "mae": mae,
            "r2_score": r2,
            "recommendation": recommendation_text,
            "chart_base64": result["chart_base64"]
        }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500
    
@api.route("/results", methods=["GET"])
def get_results():
    user_id = request.args.get("user_id")

    if not user_id:
        return jsonify({"error": "–ù–µ –≤–∫–∞–∑–∞–Ω–æ user_id"}), 400

    datasets = Dataset.query.filter_by(user_id=user_id).all()

    if not datasets:
        return jsonify({"message": "–ù–µ–º–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –¥–ª—è —Ü—å–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞"}), 200

    output = []

    for dataset in datasets:
        prediction = Prediction.query.filter_by(dataset_id=dataset.id).order_by(Prediction.created_at.desc()).first()
        if not prediction:
            continue

        recommendation = Recommendation.query.filter_by(prediction_id=prediction.id).first()

        output.append({
            "dataset_id": dataset.id,
            "filename": dataset.filename,
            "uploaded": dataset.upload_date,
            "prediction_accuracy": prediction.result_summary,
            "predicted_at": prediction.created_at,
            "recommendation": recommendation.text if recommendation else "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –≤—ñ–¥—Å—É—Ç–Ω—ñ"
        })

    return jsonify(output), 200

@api.route("/clean/<int:dataset_id>", methods=["POST"])
def clean_data(dataset_id):
    dataset = Dataset.query.get(dataset_id)

    if not dataset:
        return jsonify({"error": "–ù–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"}), 404

    filepath = os.path.join(current_app.config["UPLOAD_FOLDER"], dataset.filename)

    try:
        df = pd.read_csv(filepath)

        # –û—á–∏—â–µ–Ω–Ω—è: –≤–∏–¥–∞–ª–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ —ñ –ø–æ—Ä–æ–∂–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å
        df.drop_duplicates(inplace=True)
        df.dropna(inplace=True)

        # –ü–µ—Ä–µ–∑–∞–ø–∏—Å –æ—á–∏—â–µ–Ω–æ–≥–æ —Ñ–∞–π–ª—É
        df.to_csv(filepath, index=False)

        return jsonify({"message": "–î–∞–Ω—ñ –æ—á–∏—â–µ–Ω–æ"}), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500
    
@api.route("/history", methods=["GET"])
def get_history():
    user_id = request.args.get("user_id")
    query = request.args.get("q", "").lower()

    if not user_id:
        return jsonify({"error": "–ù–µ –≤–∫–∞–∑–∞–Ω–æ user_id"}), 400

    datasets = Dataset.query.filter_by(user_id=user_id).all()
    history = []

    for dataset in datasets:
        prediction = Prediction.query.filter_by(dataset_id=dataset.id).order_by(Prediction.created_at.desc()).first()
        topsis = TopsisResult.query.filter_by(dataset_id=dataset.id).first()
        classification_entries = ClassificationResult.query.filter_by(dataset_id=dataset.id).all()

        if prediction:
            method = "prophet"
            accuracy = prediction.result_summary
            recommendation = Recommendation.query.filter_by(prediction_id=prediction.id).first()
            recommendation_text = recommendation.text if recommendation else "N/A"
        elif topsis:
            method = "topsis"
            accuracy = "N/A"
            recommendation_text = topsis.result_text
        elif classification_entries:
            method = "classification"
            accuracy = "‚Äî"
            recommendation_text = "–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: " + ", ".join([
                f"{c.product_name} ‚Üí {'–í–∏–≥—ñ–¥–Ω–∏–π' if c.predicted_profitable else '–ù–µ–≤–∏–≥—ñ–¥–Ω–∏–π'}"
                for c in classification_entries
            ])
        else:
            method = "N/A"
            accuracy = "N/A"
            recommendation_text = "N/A"

        item = {
            "filename": dataset.filename,
            "uploaded_at": dataset.upload_date.strftime("%Y-%m-%d %H:%M:%S"),
            "accuracy": accuracy,
            "recommendation": recommendation_text,
            "dataset_id": dataset.id,
            "method": method
        }


        # –ü–æ—à—É–∫: —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞ —Ç–µ–∫—Å—Ç–æ–º
        combined_text = f"{item['filename']} {item['accuracy']} {item['recommendation']}".lower()
        if query in combined_text:
            history.append(item)
        elif not query:  # —è–∫—â–æ –ø–æ—à—É–∫–æ–≤–∏–π —Ä—è–¥–æ–∫ –ø–æ—Ä–æ–∂–Ω—ñ–π ‚Äî –ø–æ–∫–∞–∑–∞—Ç–∏ –≤—Å—ñ
            history.append(item)

    return jsonify(history), 200

@api.route("/history/<int:dataset_id>", methods=["DELETE"])
def delete_history_item(dataset_id):
    dataset = Dataset.query.get(dataset_id)
    if not dataset:
        return jsonify({"error": "–ù–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"}), 404

    # –í–∏–¥–∞–ª–µ–Ω–Ω—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π
    predictions = Prediction.query.filter_by(dataset_id=dataset_id).all()
    for prediction in predictions:
        Recommendation.query.filter_by(prediction_id=prediction.id).delete()

    # –í–∏–¥–∞–ª–µ–Ω–Ω—è –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤
    Prediction.query.filter_by(dataset_id=dataset_id).delete()

    # –í–∏–¥–∞–ª–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É
    db.session.delete(dataset)
    db.session.commit()

    return jsonify({"message": "–ü—Ä–æ–≥–Ω–æ–∑ —É—Å–ø—ñ—à–Ω–æ –≤–∏–¥–∞–ª–µ–Ω–æ"}), 200

@api.route("/profile/<int:user_id>", methods=["PUT"])
def update_profile(user_id):
    user = User.query.get(user_id)
    if not user:
        return jsonify({"error": "–ö–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"}), 404

    data = request.json
    username = data.get("username")
    email = data.get("email")
    password = data.get("password")

    if username:
        user.username = username
    if email:
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —á–∏ —Ç–∞–∫–∏–π email –≤–∂–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —ñ–Ω—à–∏–º –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º
        existing_user = User.query.filter(User.email == email, User.id != user_id).first()
        if existing_user:
            return jsonify({"error": "–¶–µ–π email –≤–∂–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —ñ–Ω—à–∏–º –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º"}), 409
        user.email = email
    if password:
        user.password = generate_password_hash(password)

    db.session.commit()
    return jsonify({"message": "–ü—Ä–æ—Ñ—ñ–ª—å —É—Å–ø—ñ—à–Ω–æ –æ–Ω–æ–≤–ª–µ–Ω–æ"}), 200

@api.route("/profile/<int:user_id>", methods=["GET"])
def get_profile(user_id):
    user = User.query.get(user_id)
    if not user:
        return jsonify({"error": "–ö–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"}), 404

    return jsonify({
        "username": user.username,
        "email": user.email
    }), 200

import pandas as pd

from fpdf import FPDF
import io
from flask import send_file

from flask import request, jsonify, send_file, current_app
from app.models import Dataset, Prediction, Recommendation, TopsisResult, ClassificationResult, SalesPrediction, SalesRecommendation
from fpdf import FPDF
import pandas as pd
import os
import io

@api.route("/export/<int:dataset_id>", methods=["GET"])
def export_prediction(dataset_id):
    export_format = request.args.get("format", "pdf")
    dataset = Dataset.query.get(dataset_id)
    if not dataset:
        return jsonify({"error": "–ù–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"}), 404

    prediction = Prediction.query.filter_by(dataset_id=dataset_id).first()
    topsis = TopsisResult.query.filter_by(dataset_id=dataset_id).first()
    classification = ClassificationResult.query.filter_by(dataset_id=dataset_id).all()
    sales_predictions = SalesPrediction.query.filter_by(dataset_id=dataset_id).all()
    recommendations = SalesRecommendation.query.filter_by(dataset_id=dataset_id).all()
    rec_map = {r.category: r.recommendation for r in recommendations}

    filename = dataset.filename
    uploaded_at = dataset.upload_date.strftime("%Y-%m-%d %H:%M:%S")

    if topsis and not classification:
        if export_format == "excel":
            return jsonify({"error": "–§–æ—Ä–º–∞—Ç Excel –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π –¥–ª—è TOPSIS"}), 400

        # TOPSIS ‚Äî –ª–∏—à–µ PDF –∑ —Ç–µ–∫—Å—Ç–æ–º result_text
        pdf = FPDF()
        font_path = os.path.join("fonts", "DejaVuSans.ttf")
        pdf.add_font("DejaVu", "", font_path, uni=True)
        pdf.set_font("DejaVu", size=12)
        pdf.add_page()
        pdf.cell(200, 10, txt="–ó–≤—ñ—Ç TOPSIS", ln=True, align='C')
        pdf.ln(10)
        pdf.cell(200, 10, txt=f"–§–∞–π–ª: {filename}", ln=True)
        pdf.cell(200, 10, txt=f"–î–∞—Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {uploaded_at}", ln=True)
        pdf.multi_cell(0, 10, txt=f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è: {topsis.result_text}")

        buffer = io.BytesIO(pdf.output(dest='S').encode('latin1'))
        buffer.seek(0)
        return send_file(buffer, as_attachment=True, download_name="topsis_result.pdf", mimetype="application/pdf")

    if classification:
        rows = [{
            "–¢–æ–≤–∞—Ä": c.product_name,
            "–†–µ–∑—É–ª—å—Ç–∞—Ç": "–í–∏–≥—ñ–¥–Ω–∏–π" if c.predicted_profitable else "–ù–µ–≤–∏–≥—ñ–¥–Ω–∏–π"
        } for c in classification]

        if export_format == "excel":
            df = pd.DataFrame(rows)
            buffer = io.BytesIO()
            df.to_excel(buffer, index=False, engine='openpyxl')
            buffer.seek(0)
            return send_file(
                buffer,
                as_attachment=True,
                download_name="classification.xlsx",
                mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

        elif export_format == "pdf":
            pdf = FPDF()
            font_path = os.path.join("fonts", "DejaVuSans.ttf")
            pdf.add_font("DejaVu", "", font_path, uni=True)
            pdf.set_font("DejaVu", size=12)
            pdf.add_page()
            pdf.cell(200, 10, txt="–ó–≤—ñ—Ç –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó", ln=True, align='C')
            pdf.ln(10)
            pdf.cell(200, 10, txt=f"–§–∞–π–ª: {filename}", ln=True)
            pdf.cell(200, 10, txt=f"–î–∞—Ç–∞: {uploaded_at}", ln=True)
            pdf.ln(5)
            for row in rows:
                pdf.cell(200, 10, txt=f"{row['–¢–æ–≤–∞—Ä']}: {row['–†–µ–∑—É–ª—å—Ç–∞—Ç']}", ln=True)

            buffer = io.BytesIO(pdf.output(dest='S').encode('latin1'))
            buffer.seek(0)
            return send_file(buffer, as_attachment=True, download_name="classification.pdf", mimetype="application/pdf")

    if prediction:
        accuracy = prediction.result_summary
        recommendation = Recommendation.query.filter_by(prediction_id=prediction.id).first()
        recommendation_text = recommendation.text if recommendation else "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è –≤—ñ–¥—Å—É—Ç–Ω—è"

        # –ü–µ—Ä—à–∏–π –ø—Ä–æ–≥–Ω–æ–∑ Prophet –∑ forecast —Ç–∞–±–ª–∏—Ü–µ—é
        forecast_rows = [
            {
                "–î–∞—Ç–∞": p.predicted_date.strftime("%Y-%m-%d"),
                "–ö–∞—Ç–µ–≥–æ—Ä—ñ—è": p.category,
                "–ü—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω—ñ –ø—Ä–æ–¥–∞–∂—ñ": p.predicted_sales,
                "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è": rec_map.get(p.category, "‚Äî")
            }
            for p in sales_predictions
        ]

        if export_format == "excel":
            df = pd.DataFrame(forecast_rows)
            buffer = io.BytesIO()
            df.to_excel(buffer, index=False, engine='openpyxl')
            buffer.seek(0)
            return send_file(
                buffer,
                as_attachment=True,
                download_name="forecast.xlsx",
                mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

        elif export_format == "pdf":
            pdf = FPDF()
            font_path = os.path.join("fonts", "DejaVuSans.ttf")
            pdf.add_font("DejaVu", "", font_path, uni=True)
            pdf.set_font("DejaVu", size=12)
            pdf.add_page()
            pdf.cell(200, 10, txt="–ó–≤—ñ—Ç –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è (Prophet)", ln=True, align='C')
            pdf.ln(10)
            pdf.cell(200, 10, txt=f"–§–∞–π–ª: {filename}", ln=True)
            pdf.cell(200, 10, txt=f"–î–∞—Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {uploaded_at}", ln=True)
            pdf.cell(200, 10, txt=f"–¢–æ—á–Ω—ñ—Å—Ç—å: {accuracy}", ln=True)
            pdf.multi_cell(0, 10, txt=f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è: {recommendation_text}")
            pdf.ln(5)
            for row in forecast_rows:
                pdf.cell(200, 10, txt=f"{row['–î–∞—Ç–∞']} ‚Äî {row['–ö–∞—Ç–µ–≥–æ—Ä—ñ—è']}: {row['–ü—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω—ñ –ø—Ä–æ–¥–∞–∂—ñ']} ({row['–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è']})", ln=True)

            buffer = io.BytesIO(pdf.output(dest='S').encode('latin1'))
            buffer.seek(0)
            return send_file(buffer, as_attachment=True, download_name="forecast.pdf", mimetype="application/pdf")

    return jsonify({"error": "–ù–µ–º–∞—î –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –µ–∫—Å–ø–æ—Ä—Ç—É"}), 400
    
from app.sales_forecast import predict_sales
from app.models import SalesPrediction, Dataset

@api.route("/forecast/<int:dataset_id>", methods=["POST"])
def forecast_sales(dataset_id):
    dataset = Dataset.query.get(dataset_id)
    if not dataset:
        return jsonify({"error": "–ù–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"}), 404

    filepath = os.path.join(current_app.config["UPLOAD_FOLDER"], dataset.filename)
    try:
        forecast = predict_sales(filepath)

        # –ì—Ä—É–ø—É—î–º–æ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö
        category_summary = {}
        for item in forecast:
            cat = item["category"]
            category_summary.setdefault(cat, []).append(item["predicted_sales"])

        for item in forecast:
            entry = SalesPrediction(
                dataset_id=dataset_id,
                predicted_date=item["predicted_date"],
                predicted_sales=item["predicted_sales"],
                category=item["category"]
            )
            db.session.add(entry)
        db.session.commit()

        for category, values in category_summary.items():
            avg = sum(values) / len(values)
            if avg >= 13:
                rec_text = "–í–∞—Ä—Ç–æ –ø–æ–ø–æ–≤–Ω–∏—Ç–∏ —Å–∫–ª–∞–¥"
            elif avg < 10:
                rec_text = "–ó–Ω–∏–∂–µ–Ω–Ω—è –ø–æ–ø–∏—Ç—É, –∑–º–µ–Ω—à—ñ—Ç—å –∑–∞–ø–∞—Å–∏"
            else:
                rec_text = "–°—Ç–∞–±—ñ–ª—å–Ω–∏–π –ø–æ–ø–∏—Ç"

            rec_entry = SalesRecommendation(
                dataset_id=dataset_id,
                category=category,
                recommendation=rec_text
            )
            db.session.add(rec_entry)

        db.session.commit()
        return jsonify({"message": "–ü—Ä–æ–≥–Ω–æ–∑ —É—Å–ø—ñ—à–Ω–æ —Å—Ç–≤–æ—Ä–µ–Ω–æ", "forecast": forecast})

    except Exception as e:
        return jsonify({"error": str(e)}), 500
    
@api.route("/forecast_data/<int:dataset_id>", methods=["GET"])
def get_forecast_data(dataset_id):
    predictions = SalesPrediction.query.filter_by(dataset_id=dataset_id).all()

    result = [
        {
            "date": pred.predicted_date.strftime("%Y-%m-%d"),
            "sales": pred.predicted_sales,
            "category": pred.category
        }
        for pred in predictions
    ]

    return jsonify(result), 200

from app.prophet_forecast import generate_forecast
from app.models import SalesRecommendation
@api.route("/real_forecast/<int:dataset_id>", methods=["POST"])
def real_forecast(dataset_id):
    dataset = Dataset.query.get(dataset_id)
    if not dataset:
        return jsonify({"error": "–ù–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"}), 404

    filepath = os.path.join(current_app.config["UPLOAD_FOLDER"], dataset.filename)

    try:
        # üßπ –û—á–∏—Å—Ç–∏—Ç–∏ —Å—Ç–∞—Ä—ñ –ø—Ä–æ–≥–Ω–æ–∑–∏
        SalesPrediction.query.filter_by(dataset_id=dataset_id).delete()
        SalesRecommendation.query.filter_by(dataset_id=dataset_id).delete()
        db.session.commit()

        # üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ CSV
        df = pd.read_csv(filepath)

        # ‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ (–≤—Å—é —Ä–µ—à—Ç—É ‚Äî –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ generate_forecast)
        df.columns = df.columns.str.strip().str.lower()
        required = {"date", "category", "sales"}
        if not required.issubset(df.columns):
            return jsonify({"error": f"–§–∞–π–ª –ø–æ–≤–∏–Ω–µ–Ω –º—ñ—Å—Ç–∏—Ç–∏ –∫–æ–ª–æ–Ω–∫–∏: {required}"}), 400

        # üîÆ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø—Ä–æ–≥–Ω–æ–∑—É
        forecast_list = generate_forecast(df)

        # üìä –ì—Ä—É–ø—É–≤–∞–Ω–Ω—è –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π
        category_summary = {}
        for item in forecast_list:
            cat = item["category"]
            category_summary.setdefault(cat, []).append(item["predicted_sales"])

        # üíæ –ó–±–µ—Ä–µ–≥—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑
        for item in forecast_list:
            db.session.add(SalesPrediction(
                dataset_id=dataset_id,
                predicted_date=item["date"],
                predicted_sales=item["predicted_sales"],
                category=item["category"]
            ))

        # üí¨ –ó–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
        for category, values in category_summary.items():
            avg = sum(values) / len(values)
            if avg >= 13:
                rec_text = "–í–∞—Ä—Ç–æ –ø–æ–ø–æ–≤–Ω–∏—Ç–∏ —Å–∫–ª–∞–¥"
            elif avg < 10:
                rec_text = "–ó–Ω–∏–∂–µ–Ω–Ω—è –ø–æ–ø–∏—Ç—É, –∑–º–µ–Ω—à—ñ—Ç—å –∑–∞–ø–∞—Å–∏"
            else:
                rec_text = "–°—Ç–∞–±—ñ–ª—å–Ω–∏–π –ø–æ–ø–∏—Ç"

            db.session.add(SalesRecommendation(
                dataset_id=dataset_id,
                category=category,
                recommendation=rec_text
            ))

        db.session.commit()

        return jsonify({"message": "–ü—Ä–æ–≥–Ω–æ–∑ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ", "forecast": forecast_list}), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500



    
@api.route("/forecast_results/<int:dataset_id>", methods=["GET"])
def get_forecast_results(dataset_id):  # <--- —ñ–Ω—à–∞ –Ω–∞–∑–≤–∞
    predictions = SalesPrediction.query.filter_by(dataset_id=dataset_id).all()
    recs = SalesRecommendation.query.filter_by(dataset_id=dataset_id).all()
    rec_map = {r.category: r.recommendation for r in recs}

    result = []
    for p in predictions:
        result.append({
            "date": p.predicted_date.strftime("%Y-%m-%d"),
            "category": p.category,
            "sales": p.predicted_sales,
            "recommendation": rec_map.get(p.category, "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è –≤—ñ–¥—Å—É—Ç–Ω—è")
        })

    return jsonify(result), 200
@api.route("/combined_forecast/<int:dataset_id>", methods=["GET"])
def combined_forecast(dataset_id):
    import traceback

    dataset = Dataset.query.get(dataset_id)
    if not dataset:
        return jsonify({"error": "–ù–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"}), 404

    filepath = os.path.join(current_app.config["UPLOAD_FOLDER"], dataset.filename)
    try:
        df = pd.read_csv(filepath)

        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –Ω–∞–∑–≤–∏ –∫–æ–ª–æ–Ω–æ–∫
        df.columns = df.columns.str.strip().str.lower()

        # –ü–µ—Ä–µ–π–º–µ–Ω–æ–≤—É—î–º–æ value ‚Üí sales (–ø—ñ—Å–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó!)
        if "value" in df.columns:
            df.rename(columns={"value": "sales"}, inplace=True)

        # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏—á–Ω–∏–π –≤–∏–≤—ñ–¥:
        print("DF COLUMNS:", df.columns.tolist())

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_cols = {"date", "category", "sales"}
        if not required_cols.issubset(df.columns):
            return jsonify({"error": f"CSV –º–∞—î –º—ñ—Å—Ç–∏—Ç–∏ –∫–æ–ª–æ–Ω–∫–∏: {required_cols}"}), 400

        # –î–∞—Ç–∞
        df["date"] = pd.to_datetime(df["date"])



        # –§–∞–∫—Ç–∏—á–Ω—ñ –¥–∞–Ω—ñ
        actual_data = {}
        for cat in df["category"].unique():
            cat_data = df[df["category"] == cat][["date", "sales"]]
            actual_data[cat] = [
                {"date": d.strftime("%Y-%m-%d"), "sales": float(s)}
                for d, s in zip(cat_data["date"], cat_data["sales"])
            ]

        # –ü—Ä–æ–≥–Ω–æ–∑
        forecast_data = generate_forecast(df)  # [{date, category, predicted_sales}...]

        forecast_grouped = {}
        for item in forecast_data:
            forecast_grouped.setdefault(item["category"], []).append({
                "date": item["date"],
                "sales": item["predicted_sales"]
            })

        # –û–± º—î–¥–Ω–∞–Ω–Ω—è
        result = []
        for cat in actual_data:
            result.append({
                "category": cat,
                "actual": actual_data.get(cat, []),
                "forecast": forecast_grouped.get(cat, [])
            })

        return jsonify(result), 200

    except Exception as e:
        print("=== –ü–û–ú–ò–õ–ö–ê ===")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

from flask import Blueprint, request, jsonify, current_app
import os
import pandas as pd
from werkzeug.utils import secure_filename
from app import db
from app.models import Dataset, TopsisResult
from utils.topsis import run_topsis

topsis_bp = Blueprint("topsis", __name__)

@api.route("/topsis", methods=["POST"])
def topsis_analysis():
    if "file" not in request.files:
        return jsonify({"error": "–§–∞–π–ª –Ω–µ –Ω–∞–¥—ñ—Å–ª–∞–Ω–æ"}), 400

    file = request.files["file"]
    user_id = request.form.get("user_id")

    if not user_id:
        return jsonify({"error": "–ù–µ –≤–∫–∞–∑–∞–Ω–æ user_id"}), 400
    if file.filename == "":
        return jsonify({"error": "–§–∞–π–ª –Ω–µ –≤–∏–±—Ä–∞–Ω–æ"}), 400

    try:
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ–∞–π–ª
        filename = secure_filename(file.filename)
        upload_path = os.path.join(current_app.config["UPLOAD_FOLDER"], filename)
        file.save(upload_path)

        # –î–æ–¥–∞—î–º–æ –¥–æ –±–∞–∑–∏
        dataset = Dataset(
            user_id=user_id,
            filename=filename,
            upload_date=datetime.utcnow()
        )
        db.session.add(dataset)
        db.session.commit()

        # –û–±—Ä–æ–±–∫–∞ —á–µ—Ä–µ–∑ TOPSIS
        df = pd.read_csv(upload_path)
        result_text = run_topsis(df)

        topsis_result = TopsisResult(
            dataset_id=dataset.id,
            result_text=result_text
        )
        db.session.add(topsis_result)
        db.session.commit()

        return jsonify({
            "message": "TOPSIS –∞–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ",
            "result": result_text
        }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@api.route("/classify", methods=["POST"])
def classify_products():
    from app.models import Dataset, ClassificationResult
    from app.classifier import run_classification

    if "file" not in request.files:
        return jsonify({"error": "–§–∞–π–ª –Ω–µ –Ω–∞–¥—ñ—Å–ª–∞–Ω–æ"}), 400

    file = request.files["file"]
    user_id = request.form.get("user_id")

    if not user_id:
        return jsonify({"error": "–ù–µ –≤–∫–∞–∑–∞–Ω–æ user_id"}), 400
    if file.filename == "":
        return jsonify({"error": "–§–∞–π–ª –Ω–µ –≤–∏–±—Ä–∞–Ω–æ"}), 400

    try:
        filename = secure_filename(file.filename)
        upload_path = os.path.join(current_app.config["UPLOAD_FOLDER"], filename)
        file.save(upload_path)

        dataset = Dataset(user_id=user_id, filename=filename, upload_date=datetime.utcnow())
        db.session.add(dataset)
        db.session.commit()

        df = pd.read_csv(upload_path)
        result_table, metrics = run_classification(df)

        for item in result_table:
            db.session.add(ClassificationResult(
                dataset_id=dataset.id,
                product_name=item["product"],
                predicted_profitable=bool(item["predicted"]),
                created_at=datetime.utcnow()
            ))
        db.session.commit()

        return jsonify({
            "message": "–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞",
            "results": result_table,
            "metrics": metrics
        }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500
